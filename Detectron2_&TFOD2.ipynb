{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8BDsI5_4gAx"
      },
      "source": [
        "Assignment Code: DS-AG-024\n",
        "\n",
        "Detectron2 & TFOD2 | Assignment\n",
        "\n",
        "Question 1: What is Detectron2 and how does it differ from previous object detection\n",
        "frameworks?\n",
        "\n",
        "Answer:Detectron2 is an open-source, next-generation computer vision library developed by Facebook AI Research (FAIR) for object detection, instance segmentation, semantic segmentation, keypoint detection, and panoptic segmentation. It is written in PyTorch and is designed to be modular, flexible, and high-performance, making it suitable for both research and production use.\n",
        "\n",
        "Detectron2 provides:\n",
        "\n",
        "State-of-the-art implementations of modern detection models (e.g., Faster R-CNN, Mask R-CNN, RetinaNet, Cascade R-CNN)\n",
        "\n",
        "Pre-trained models and standardized evaluation tools\n",
        "\n",
        "A clean API for training, inference, and experimentation\n",
        "\n",
        "GPU-accelerated operations optimized for speed and scalability\n",
        "\n",
        "It is widely used in academic research and industry for building and benchmarking object detection systems.\n",
        "\n",
        "How Detectron2 Differs from Previous Object Detection Frameworks -\n",
        "\n",
        "1.Modern PyTorch-Based Architecture\n",
        "\n",
        "Detectron2 is fully built on PyTorch, enabling:\n",
        "\n",
        "Dynamic computation graphs\n",
        "\n",
        "Easier debugging\n",
        "\n",
        "Native support for PyTorch tools and ecosystem\n",
        "\n",
        "Earlier frameworks (e.g., original Detectron) were based on Caffe2, which was harder to modify and debug.\n",
        "\n",
        "2.Highly Modular and Extensible Design\n",
        "\n",
        "Components such as:\n",
        "\n",
        "Backbone networks\n",
        "\n",
        "Region Proposal Networks (RPNs)\n",
        "\n",
        "ROI heads\n",
        "\n",
        "Loss functions\n",
        "are decoupled and configurable\n",
        "\n",
        "Previous frameworks often had tightly coupled pipelines, making customization difficult.\n",
        "\n",
        "3.Improved Performance and Efficiency\n",
        "\n",
        "Optimized CUDA operators and memory usage\n",
        "\n",
        "Faster training and inference compared to older frameworks\n",
        "\n",
        "Supports mixed-precision training and distributed training out of the box\n",
        "\n",
        "4.Support for Advanced Vision Tasks\n",
        "\n",
        "Detectron2 natively supports:\n",
        "\n",
        "Object detection\n",
        "\n",
        "Instance segmentation\n",
        "\n",
        "Semantic segmentation\n",
        "\n",
        "Panoptic segmentation\n",
        "\n",
        "Keypoint detection\n",
        "\n",
        "Older frameworks typically focused only on bounding-box detection.\n",
        "\n",
        "5.Research-Friendly Features\n",
        "\n",
        "Clean configuration system (YAML-based)\n",
        "\n",
        "Easy experimentation with new architectures\n",
        "\n",
        "Built-in visualization, logging, and evaluation tools (COCO, LVIS, etc.)\n",
        "\n",
        "Easier integration of custom datasets and models\n",
        "\n",
        "Earlier frameworks required significant boilerplate code for experimentation.\n",
        "\n",
        "6.Better Dataset and Evaluation Handling\n",
        "\n",
        "Standardized dataset registration\n",
        "\n",
        "Unified evaluation pipeline\n",
        "\n",
        "Supports multiple benchmarks seamlessly\n",
        "Older frameworks often required dataset-specific custom code.\n",
        "\n",
        "7.Active Development and Community Support\n",
        "\n",
        "Regular updates reflecting the latest research\n",
        "\n",
        "Strong documentation and examples\n",
        "\n",
        "Broad adoption in both academia and industry\n",
        "Many older frameworks are now deprecated or poorly maintained.\n",
        "\n",
        "Question 2: Explain the process and importance of data annotation when working with\n",
        "Detectron2.\n",
        "\n",
        "Answer:Data annotation is the process of labeling raw images with structured information (such as bounding boxes, class labels, masks, or keypoints) so that Detectron2 models can learn to recognize and localize objects. Since Detectron2 is a supervised learning framework, high-quality annotated data is essential for training accurate models.\n",
        "\n",
        "Process of Data Annotation for Detectron2 -\n",
        "\n",
        "1.Understanding the Task Requirements\n",
        "\n",
        "The annotation type depends on the vision task:\n",
        "\n",
        "Object Detection → Bounding boxes + class labels\n",
        "\n",
        "Instance Segmentation → Pixel-level masks for each object\n",
        "\n",
        "Semantic Segmentation → Class label for every pixel\n",
        "\n",
        "Keypoint Detection → Coordinates of body joints or landmarks\n",
        "\n",
        "Choosing the correct annotation format ensures compatibility with the model architecture.\n",
        "\n",
        "2.Collecting and Preparing the Dataset\n",
        "\n",
        "Gather representative images covering:\n",
        "\n",
        "Different object sizes\n",
        "\n",
        "Lighting conditions\n",
        "\n",
        "Occlusions and viewpoints\n",
        "\n",
        "Clean the dataset by removing corrupted or irrelevant images\n",
        "\n",
        "A diverse dataset improves generalization.\n",
        "\n",
        "3.Annotating the Images\n",
        "\n",
        "Annotations are typically created using tools such as:\n",
        "\n",
        "LabelImg (bounding boxes)\n",
        "\n",
        "CVAT or LabelMe (segmentation masks)\n",
        "\n",
        "Supervisely or Roboflow (multi-task annotation)\n",
        "\n",
        "Each image is labeled with:\n",
        "\n",
        "Object class IDs\n",
        "\n",
        "Bounding box coordinates\n",
        "\n",
        "Polygon masks or keypoints (if required)\n",
        "\n",
        "4.Converting Annotations to Detectron2 Format\n",
        "\n",
        "Detectron2 primarily uses COCO-style JSON annotations, which include:\n",
        "\n",
        "Image metadata\n",
        "\n",
        "Object categories\n",
        "\n",
        "Annotations (boxes, masks, keypoints)\n",
        "\n",
        "Annotations must be:\n",
        "\n",
        "Correctly indexed\n",
        "\n",
        "Consistent with class definitions\n",
        "\n",
        "Free from overlaps or missing labels\n",
        "\n",
        "5.Dataset Registration in Detectron2\n",
        "\n",
        "Annotated datasets are registered using Detectron2’s dataset API\n",
        "\n",
        "Metadata such as class names and color maps are defined\n",
        "\n",
        "This step allows Detectron2 to load, visualize, train, and evaluate models correctly\n",
        "\n",
        "6.Verification and Quality Control\n",
        "\n",
        "Visual inspection of annotations\n",
        "\n",
        "Checking class balance\n",
        "\n",
        "Fixing incorrect or inconsistent labels\n",
        "\n",
        "Poor annotation quality directly degrades model performance.\n",
        "\n",
        "Importance of Data Annotation in Detectron2 -\n",
        "\n",
        "1.Foundation for Model Learning\n",
        "\n",
        "Detectron2 models learn patterns only from annotated data. Incorrect labels lead to incorrect predictions.\n",
        "\n",
        "2.Improves Model Accuracy and Robustness\n",
        "\n",
        "Precise bounding boxes improve localization\n",
        "\n",
        "Accurate masks improve segmentation quality\n",
        "\n",
        "Correct keypoints improve pose estimation\n",
        "\n",
        "3.Reduces Overfitting and Bias\n",
        "\n",
        "Well-annotated and diverse datasets help models generalize to unseen data and reduce class bias.\n",
        "\n",
        "4.Ensures Proper Evaluation\n",
        "\n",
        "Reliable annotations enable:\n",
        "\n",
        "Accurate loss calculation\n",
        "\n",
        "Correct performance metrics (mAP, IoU)\n",
        "\n",
        "Fair comparison between models\n",
        "\n",
        "5.Supports Advanced Tasks\n",
        "\n",
        "High-quality annotations allow Detectron2 to handle:\n",
        "\n",
        "Multi-class detection\n",
        "\n",
        "Panoptic segmentation\n",
        "\n",
        "Fine-grained object recognition.\n",
        "\n",
        "Question 3: Describe the steps involved in training a custom object detection model\n",
        "using Detectron2.\n",
        "\n",
        "Answer:Steps Involved in Training a Custom Object Detection Model\n",
        "1.Install and Set Up Detectron2\n",
        "\n",
        "Install Detectron2 with compatible versions of:\n",
        "\n",
        "Python\n",
        "\n",
        "PyTorch\n",
        "\n",
        "CUDA (for GPU acceleration)\n",
        "\n",
        "Verify installation using provided demo scripts\n",
        "This ensures the environment is ready for training.\n",
        "\n",
        "2.Prepare the Custom Dataset\n",
        "\n",
        "Collect images relevant to the detection task\n",
        "\n",
        "Ensure dataset diversity (angles, lighting, object sizes)\n",
        "\n",
        "Split the dataset into:\n",
        "\n",
        "Training set\n",
        "\n",
        "Validation set (and optionally test set)\n",
        "\n",
        "A well-prepared dataset improves model generalization.\n",
        "\n",
        "3.Annotate the Dataset\n",
        "\n",
        "Label objects using bounding boxes and class names\n",
        "\n",
        "Use annotation tools to generate structured labels\n",
        "\n",
        "Save annotations in COCO-style JSON format, which Detectron2 natively supports\n",
        "\n",
        "Accurate annotation is critical for effective learning.\n",
        "\n",
        "4.Register the Dataset in Detectron2\n",
        "\n",
        "Register training and validation datasets using Detectron2’s dataset API\n",
        "\n",
        "Define:\n",
        "\n",
        "Dataset name\n",
        "\n",
        "Annotation file path\n",
        "\n",
        "Image directory\n",
        "\n",
        "Specify metadata such as class names\n",
        "\n",
        "This step allows Detectron2 to load and visualize the data correctly.\n",
        "\n",
        "5.Select a Base Model and Configuration\n",
        "\n",
        "Choose a pre-trained detection model (e.g., Faster R-CNN, RetinaNet)\n",
        "\n",
        "Load its configuration file\n",
        "\n",
        "Modify configuration parameters such as:\n",
        "\n",
        "Number of classes\n",
        "\n",
        "Learning rate\n",
        "\n",
        "Batch size\n",
        "\n",
        "Number of training iterations\n",
        "\n",
        "Using pre-trained weights enables transfer learning, reducing training time.\n",
        "\n",
        "6.Configure the Training Process\n",
        "\n",
        "Set output directories for model checkpoints\n",
        "\n",
        "Define evaluation intervals\n",
        "\n",
        "Enable GPU or multi-GPU training if available\n",
        "\n",
        "Adjust solver settings (optimizer, scheduler)\n",
        "\n",
        "Proper configuration ensures stable and efficient training.\n",
        "\n",
        "7.Train the Model\n",
        "\n",
        "Start training using Detectron2’s training engine\n",
        "\n",
        "The model learns to:\n",
        "\n",
        "Propose regions\n",
        "\n",
        "Classify objects\n",
        "\n",
        "Refine bounding boxes\n",
        "\n",
        "Training loss and metrics are logged for monitoring\n",
        "\n",
        "8.Evaluate the Model\n",
        "\n",
        "Run evaluation on the validation dataset\n",
        "\n",
        "Measure performance using metrics such as:\n",
        "\n",
        "Mean Average Precision (mAP)\n",
        "\n",
        "Intersection over Union (IoU)\n",
        "\n",
        "Analyze errors such as false positives or missed detections\n",
        "\n",
        "Evaluation helps determine model effectiveness.\n",
        "\n",
        "9.Inference and Testing\n",
        "\n",
        "Use the trained model to perform inference on new images\n",
        "\n",
        "Visualize predicted bounding boxes and confidence scores\n",
        "\n",
        "Verify real-world performance\n",
        "\n",
        "10.Fine-Tuning and Optimization\n",
        "\n",
        "Adjust hyperparameters if performance is unsatisfactory\n",
        "\n",
        "Improve dataset quality or balance classes\n",
        "\n",
        "Retrain or fine-tune the model for better accuracy.\n",
        "\n",
        "Question 4: What are evaluation curves in Detectron2, and how are metrics like mAP\n",
        "and IoU interpreted?\n",
        "\n",
        "Answer:In Detectron2, evaluation curves are graphical representations used to analyze and visualize a model’s detection performance during validation or testing. These curves help in understanding how well the trained model predicts object locations and classes across different confidence thresholds.\n",
        "\n",
        "Common evaluation curves include:\n",
        "\n",
        "Precision–Recall (PR) curves\n",
        "\n",
        "Average Precision (AP) curves\n",
        "\n",
        "Loss curves (training vs validation.\n",
        "\n",
        "These curves are generated using standard benchmarks such as the COCO evaluation protocol, which Detectron2 supports by default.\n",
        "\n",
        "Intersection over Union (IoU)\n",
        "Definition\n",
        "\n",
        "Intersection over Union (IoU) measures how well a predicted bounding box overlaps with the ground-truth bounding box.\n",
        "\n",
        "IoU=Area of Overlap​/Area of Union\n",
        "\n",
        "Interpretation\n",
        "\n",
        "IoU = 1.0 → Perfect overlap\n",
        "\n",
        "IoU ≥ 0.5 → Common threshold for a correct detection\n",
        "\n",
        "Higher IoU → More accurate localization\n",
        "\n",
        "IoU is used:\n",
        "\n",
        "To classify detections as true positives or false positives\n",
        "\n",
        "As a threshold when computing precision, recall, and AP\n",
        "\n",
        "Precision and Recall\n",
        "\n",
        "Precision = TP/TP+FP\n",
        "\n",
        "Measures how many predicted objects are correct\n",
        "\n",
        "High precision → Few false positives.\n",
        "\n",
        "Recall\n",
        "\n",
        "Recall = TP/TP+FN\n",
        "\n",
        "Measures how many actual objects are detected\n",
        "\n",
        "High recall → Few missed objects.\n",
        "\n",
        "Precision–Recall (PR) Curve\n",
        "\n",
        "Plots precision against recall at different confidence thresholds\n",
        "\n",
        "Shows the trade-off between:\n",
        "\n",
        "Detecting more objects (recall)\n",
        "\n",
        "Avoiding false detections (precision)\n",
        "\n",
        "A curve closer to the top-right corner indicates better performance.\n",
        "\n",
        "Average Precision (AP)\n",
        "Definition\n",
        "\n",
        "Average Precision (AP) is the area under the Precision–Recall curve for a single class at a specific IoU threshold.\n",
        "\n",
        "AP summarizes the PR curve into a single value\n",
        "\n",
        "Higher AP indicates better detection quality\n",
        "\n",
        "Mean Average Precision (mAP)\n",
        "Definition\n",
        "\n",
        "Mean Average Precision (mAP) is the mean of AP values:\n",
        "\n",
        "Across all object classes\n",
        "\n",
        "Often across multiple IoU thresholds\n",
        "\n",
        "In Detectron2 (COCO-style)\n",
        "\n",
        "mAP@[0.5:0.95] → Average AP over IoU thresholds from 0.5 to 0.95\n",
        "\n",
        "AP50 → AP at IoU = 0.5\n",
        "\n",
        "AP75 → AP at IoU = 0.75\n",
        "\n",
        "Interpretation\n",
        "\n",
        "High mAP → Strong classification and localization performance\n",
        "\n",
        "Large gap between AP50 and AP75 → Good detection but poor box precision\n",
        "\n",
        "Importance of Evaluation Curves and Metrics\n",
        "1.Model Performance Assessment\n",
        "\n",
        "They provide quantitative and visual insight into detection quality.\n",
        "\n",
        "2.Hyperparameter Tuning\n",
        "\n",
        "Curves help identify:\n",
        "\n",
        "Overfitting\n",
        "\n",
        "Underfitting\n",
        "\n",
        "Optimal confidence thresholds\n",
        "\n",
        "3.Fair Model Comparison\n",
        "\n",
        "mAP enables objective comparison between different models or configurations.\n",
        "\n",
        "4.Error Analysis\n",
        "\n",
        "IoU and PR curves help diagnose:\n",
        "\n",
        "Localization errors\n",
        "\n",
        "Class confusion\n",
        "\n",
        "False positives vs missed detections.\n",
        "\n",
        "Question 5: Compare Detectron2 and TFOD2 in terms of features, performance, and\n",
        "ease of use.\n",
        "\n",
        "Answer:\n",
        "\n",
        "| Aspect                | Detectron2                                                                    | TFOD2                           |\n",
        "| --------------------- | ----------------------------------------------------------------------------- | ------------------------------- |\n",
        "| Framework             | PyTorch-based                                                                 | TensorFlow 2.x–based            |\n",
        "| Supported Tasks       | Detection, instance segmentation, semantic & panoptic segmentation, keypoints | Primarily object detection      |\n",
        "| Model Zoo             | Faster R-CNN, Mask R-CNN, RetinaNet, Cascade R-CNN                            | SSD, Faster R-CNN, EfficientDet |\n",
        "| Configuration         | Python + YAML configs                                                         | Protobuf configuration files    |\n",
        "| Research Support      | Very strong                                                                   | Moderate                        |\n",
        "| Production Deployment | TorchScript, ONNX                                                             | TensorFlow Serving, TFLite      |\n",
        "\n",
        "Performance Comparison\n",
        "Detectron2\n",
        "\n",
        "Highly optimized CUDA operations\n",
        "\n",
        "Strong performance on large datasets (COCO, LVIS)\n",
        "\n",
        "Better support for complex tasks like instance and panoptic segmentation\n",
        "\n",
        "Faster experimentation and fine-tuning in research settings\n",
        "\n",
        "TFOD2\n",
        "\n",
        "Efficient models for edge and mobile deployment\n",
        "\n",
        "Optimized for TensorFlow ecosystem\n",
        "\n",
        "Good inference speed with lightweight architectures like SSD and EfficientDet\n",
        "\n",
        "Suitable for real-time and embedded applications\n",
        "\n",
        "Ease of Use Comparison\n",
        "Detectron2\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Clean and modular PyTorch design\n",
        "\n",
        "Easier debugging due to dynamic computation graphs\n",
        "\n",
        "Strong documentation for researchers\n",
        "\n",
        "Limitations:\n",
        "\n",
        "Steeper learning curve for beginners\n",
        "\n",
        "Requires familiarity with PyTorch and COCO formats\n",
        "\n",
        "TFOD2\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Extensive tutorials and beginner-friendly guides\n",
        "\n",
        "Integrated with TensorFlow tools (Keras, TensorBoard)\n",
        "\n",
        "Easier deployment to mobile and edge devices\n",
        "\n",
        "Limitations:\n",
        "\n",
        "Complex Protobuf configuration files\n",
        "\n",
        "Customization can be cumbersome for advanced research\n",
        "\n",
        "Community and Ecosystem\n",
        "\n",
        "Detectron2 is widely adopted in academic research and advanced computer vision projects.\n",
        "\n",
        "TFOD2 has broader industry adoption, especially in production and mobile environments.\n",
        "\n",
        "| Criterion             | Detectron2 | TFOD2    |\n",
        "| --------------------- | ---------- | -------- |\n",
        "| Research Flexibility  | Excellent  | Moderate |\n",
        "| Customization         | High       | Medium   |\n",
        "| Training Speed        | Fast       | Moderate |\n",
        "| Deployment Ease       | Moderate   | High     |\n",
        "| Beginner Friendliness | Medium     | High     |\n",
        "\n",
        "Question 6: Write Python code to install Detectron2 and verify the installation.\n",
        "\n",
        "Answer:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmlFWk8x9_DN",
        "outputId": "ae7569e9-5fbd-4d20-b16b-3c908f817aa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 2.9.0+cu126\n",
            "CUDA Available: True\n",
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-k4gj1090\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-k4gj1090\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit fd27788985af0f4ca800bca563acdb700bb890e2\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (11.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.10.0)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.0.11)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.3.0)\n",
            "Collecting yacs>=0.1.8 (from detectron2==0.6)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.1.2)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (4.67.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.19.0)\n",
            "Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.3.0)\n",
            "Collecting hydra-core>=1.1 (from detectron2==0.6)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting black (from detectron2==0.6)\n",
            "  Downloading black-25.12.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.4/86.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (25.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.3)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\n",
            "Collecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from black->detectron2==0.6) (8.3.1)\n",
            "Collecting mypy-extensions>=0.4.3 (from black->detectron2==0.6)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pathspec>=0.9.0 (from black->detectron2==0.6)\n",
            "  Downloading pathspec-1.0.3-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.12/dist-packages (from black->detectron2==0.6) (4.5.1)\n",
            "Collecting pytokens>=0.3.0 (from black->detectron2==0.6)\n",
            "  Downloading pytokens-0.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (2.9.0.post0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (3.10)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (3.1.5)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard->detectron2==0.6) (4.15.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (3.0.3)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading black-25.12.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading pathspec-1.0.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.0/55.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytokens-0.3.0-py3-none-any.whl (12 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Building wheels for collected packages: detectron2, fvcore\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2: filename=detectron2-0.6-cp312-cp312-linux_x86_64.whl size=7085016 sha256=017d68d2be05457f81cb9f360b9e02cb02de6fa03af83cb3a5fc51ba73c680ee\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v7ex6vg4/wheels/d3/6e/bd/1969578f1456a6be2d6f083da65c669f450b23b8f3d1ac14c1\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=2cd965dffe28c85673118700daebd9d13d11b764d1e9084bed0fd66461aeda55\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/9f/a5/e4f5b27454ccd4596bd8b62432c7d6b1ca9fa22aef9d70a16a\n",
            "Successfully built detectron2 fvcore\n",
            "Installing collected packages: yacs, pytokens, portalocker, pathspec, mypy-extensions, iopath, hydra-core, black, fvcore, detectron2\n",
            "Successfully installed black-25.12.0 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.1.0 pathspec-1.0.3 portalocker-3.2.0 pytokens-0.3.0 yacs-0.1.8\n",
            "Detectron2 version: 0.6\n",
            "Installation Successful!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA Version: {CUDA_VERSION}\")\n",
        "print(f\"TORCH VERSION: {TORCH_VERSION}\")\n",
        "# Install detectron2\n",
        "!pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "\n",
        "# 3. Verify Installation\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "print(f\"Detectron2 version: {detectron2.__version__}\")\n",
        "print(\"Installation Successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Annotate a dataset using any tool of your choice and convert the\n",
        "annotations to COCO format for Detectron2.\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "ZPNu_AStSDNs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8da83cca"
      },
      "source": [
        "First, let's create a dummy `images_folder` and a dummy `annotations.json` file. This will allow the Detectron2 dataset registration code to run without a `FileNotFoundError`. You will need to replace these with your actual annotated dataset later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vbVHf0sdSOPC",
        "outputId": "372ca9be-a732-49b9-ed40-0cad1577c9e3"
      },
      "source": [
        "# Install Detectron2\n",
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "\n",
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Detectron2 utilities\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "\n",
        "# --- STEP 1: Register the Dataset ---\n",
        "# Replace 'my_dataset' with your preferred name\n",
        "# Replace the paths with your actual file locations in Colab\n",
        "DATASET_NAME = \"custom_coco_dataset\"\n",
        "JSON_PATH = \"/content/annotations.json\"\n",
        "IMAGE_ROOT = \"/content/images_folder/\"\n",
        "\n",
        "if DATASET_NAME in DatasetCatalog.list():\n",
        "    DatasetCatalog.remove(DATASET_NAME)\n",
        "\n",
        "register_coco_instances(DATASET_NAME, {}, JSON_PATH, IMAGE_ROOT)\n",
        "\n",
        "# --- STEP 2: Verify and Visualize ---\n",
        "metadata = MetadataCatalog.get(DATASET_NAME)\n",
        "dataset_dicts = DatasetCatalog.get(DATASET_NAME)\n",
        "\n",
        "print(f\"Successfully registered {len(dataset_dicts)} images.\")\n",
        "\n",
        "# Visualize 3 random samples to ensure annotations match images\n",
        "for d in random.sample(dataset_dicts, min(len(dataset_dicts), 3)):\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=metadata, scale=0.5)\n",
        "    out = visualizer.draw_dataset_dict(d)\n",
        "    print(f\"Image: {d['file_name']}\")\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-wdcsmruf\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-wdcsmruf\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit fd27788985af0f4ca800bca563acdb700bb890e2\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (11.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.10.0)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.0.11)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.3.0)\n",
            "Requirement already satisfied: yacs>=0.1.8 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (0.1.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.1.2)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (4.67.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.19.0)\n",
            "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (0.1.5.post20221221)\n",
            "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (0.1.9)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.3.0)\n",
            "Requirement already satisfied: hydra-core>=1.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (1.3.2)\n",
            "Requirement already satisfied: black in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (25.12.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (25.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.3)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from iopath<0.1.10,>=0.1.7->detectron2==0.6) (3.2.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from black->detectron2==0.6) (8.3.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from black->detectron2==0.6) (1.1.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from black->detectron2==0.6) (1.0.3)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.12/dist-packages (from black->detectron2==0.6) (4.5.1)\n",
            "Requirement already satisfied: pytokens>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from black->detectron2==0.6) (0.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (2.9.0.post0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (3.10)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (3.1.5)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard->detectron2==0.6) (4.15.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (3.0.3)\n",
            "[01/15 06:18:50 d2.data.datasets.coco]: Loaded 3 images in COCO format from /content/annotations.json\n",
            "Successfully registered 3 images.\n",
            "Image: /content/images_folder/dummy_image_002.jpg\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=320x240>"
            ],
            "image/png": "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\n",
            "image/jpeg": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: /content/images_folder/dummy_image_001.jpg\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=320x240>"
            ],
            "image/png": "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\n",
            "image/jpeg": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: /content/images_folder/dummy_image_003.jpg\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=320x240>"
            ],
            "image/png": "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\n",
            "image/jpeg": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b49385de",
        "outputId": "61f3e956-7f5f-4c07-d99b-347fff85e4ca"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# Define paths (these match the paths in your existing code)\n",
        "IMAGE_ROOT = \"/content/images_folder/\"\n",
        "JSON_PATH = \"/content/annotations.json\"\n",
        "\n",
        "# 1. Create dummy images directory\n",
        "os.makedirs(IMAGE_ROOT, exist_ok=True)\n",
        "\n",
        "# 2. Create some dummy images\n",
        "# Create 3 dummy images for demonstration\n",
        "image_names = [f\"dummy_image_{i+1:03d}.jpg\" for i in range(3)]\n",
        "image_paths = []\n",
        "for i, img_name in enumerate(image_names):\n",
        "    img_path = os.path.join(IMAGE_ROOT, img_name)\n",
        "    # Create a blank image with a specific color and size\n",
        "    img_height, img_width = 480, 640\n",
        "    color = (i * 50 % 255, (i * 100) % 255, (i * 150) % 255) # Different color for each image\n",
        "    dummy_img = np.full((img_height, img_width, 3), color, dtype=np.uint8)\n",
        "    cv2.imwrite(img_path, dummy_img)\n",
        "    image_paths.append(img_path)\n",
        "print(f\"Created {len(image_names)} dummy images in {IMAGE_ROOT}\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 3 dummy images in /content/images_folder/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6468284",
        "outputId": "e68b3a31-1a75-43c9-8e6d-26d3c228e052"
      },
      "source": [
        "# 3. Create a dummy COCO-format annotations JSON file\n",
        "# This example includes one category and one annotation per image\n",
        "\n",
        "annotations_data = {\n",
        "    \"images\": [],\n",
        "    \"annotations\": [],\n",
        "    \"categories\": [\n",
        "        {\"id\": 1, \"name\": \"object\", \"supercategory\": \"none\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "img_id_counter = 1\n",
        "ann_id_counter = 1\n",
        "\n",
        "for i, img_name in enumerate(image_names):\n",
        "    img_height, img_width = 480, 640 # Use the same dimensions as dummy images\n",
        "    annotations_data[\"images\"].append({\n",
        "        \"id\": img_id_counter,\n",
        "        \"width\": img_width,\n",
        "        \"height\": img_height,\n",
        "        \"file_name\": img_name\n",
        "    })\n",
        "\n",
        "    # Add a dummy bounding box annotation for each image\n",
        "    # bbox: [x_min, y_min, width, height]\n",
        "    bbox = [10 + i * 20, 20 + i * 30, 100, 100]\n",
        "    annotations_data[\"annotations\"].append({\n",
        "        \"id\": ann_id_counter,\n",
        "        \"image_id\": img_id_counter,\n",
        "        \"category_id\": 1, # Refers to the 'object' category\n",
        "        \"bbox\": bbox,\n",
        "        \"area\": bbox[2] * bbox[3],\n",
        "        \"segmentation\": [], # Empty for bounding box only\n",
        "        \"iscrowd\": 0\n",
        "    })\n",
        "    img_id_counter += 1\n",
        "    ann_id_counter += 1\n",
        "\n",
        "with open(JSON_PATH, 'w') as f:\n",
        "    json.dump(annotations_data, f, indent=4)\n",
        "\n",
        "print(f\"Created dummy annotations JSON file at {JSON_PATH}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dummy annotations JSON file at /content/annotations.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a script to download pretrained weights and configure paths for\n",
        "training in Detectron2.\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "Sf560BZNScn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultTrainer\n",
        "import os\n",
        "\n",
        "# 1. Initializing the configuration\n",
        "cfg = get_cfg()\n",
        "\n",
        "# 2. Loading a base configuration from the Model Zoo\n",
        "# Switching to Faster R-CNN for object detection only, as our dummy data has no masks\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "\n",
        "# 3. Specify the training and testing datasets\n",
        "# Make sure \"custom_coco_dataset\" matches the name used in your registration step\n",
        "cfg.DATASETS.TRAIN = (\"custom_coco_dataset\",)\n",
        "cfg.DATASETS.TEST = ()  # No evaluation during training in this simple setup\n",
        "\n",
        "# 4. Configure Pre-trained Weights\n",
        "# This line automatically downloads weights from the Model Zoo for Faster R-CNN\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        "\n",
        "# 5. Solver & Hardware Settings\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2  # Adjust based on your GPU VRAM\n",
        "cfg.SOLVER.BASE_LR = 0.00025  # A good starting point for transfer learning\n",
        "cfg.SOLVER.MAX_ITER = 300     # Number of training iterations\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # Replace with your actual number of classes\n",
        "\n",
        "# 6. Output Directory\n",
        "cfg.OUTPUT_DIR = \"./output\"\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# 7. Print verification\n",
        "print(f\"Pretrained weights will be loaded from: {cfg.MODEL.WEIGHTS}\")\n",
        "print(f\"Model will be saved to: {cfg.OUTPUT_DIR}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c5feJWKSnlk",
        "outputId": "8a026f61-9d8d-4362-86c2-18a9f43fac0c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained weights will be loaded from: https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl\n",
            "Model will be saved to: ./output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Show the steps and code to run inference using a trained Detectron2\n",
        "model on a new image.\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "gCnD0JPXTW0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "# Re-initialize cfg to ensure correct settings if this cell is run independently\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "\n",
        "# Configure with the settings used for training\n",
        "cfg.MODEL.WEIGHTS = os.path.join(\"./output\", \"model_final.pth\")\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "cfg.DATASETS.TEST = (\"custom_coco_dataset\",)\n",
        "\n",
        "# Create a predictor\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# --- Perform Inference ---\n",
        "# Create a dummy test image for inference\n",
        "test_img_path = \"/content/test_image.jpg\"\n",
        "img_height, img_width = 480, 640\n",
        "dummy_test_img = 255 * np.ones((img_height, img_width, 3), dtype=np.uint8) # White image\n",
        "# Add a simple shape to make it visually distinct\n",
        "cv2.rectangle(dummy_test_img, (50, 50), (200, 200), (0, 255, 0), -1) # Green rectangle\n",
        "cv2.imwrite(test_img_path, dummy_test_img)\n",
        "\n",
        "if os.path.exists(test_img_path):\n",
        "    im = cv2.imread(test_img_path)\n",
        "    outputs = predictor(im)\n",
        "\n",
        "    # We need to ensure metadata is loaded for visualization, even if TEST is empty in training cfg\n",
        "    # If 'custom_coco_dataset' was registered previously, its metadata should be available.\n",
        "    # If not, a minimal metadata setup would be required.\n",
        "    metadata = MetadataCatalog.get(\"custom_coco_dataset\")\n",
        "\n",
        "    v = Visualizer(im[:, :, ::-1], metadata=metadata, scale=0.8)\n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    print(f\"Inference on: {test_img_path}\")\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])\n",
        "else:\n",
        "    print(f\"Test image '{test_img_path}' not found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "e3n-_S1JVHVO",
        "outputId": "0f292945-a770-4b4c-8209-6e518d7ef8ed"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01/15 06:28:29 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from ./output/model_final.pth ...\n",
            "Inference on: /content/test_image.jpg\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=512x384>"
            ],
            "image/png": "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\n",
            "image/jpeg": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44d3f2b3",
        "outputId": "a99709f6-9b19-4949-f91b-672143f5f14e"
      },
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "\n",
        "# Create a trainer with the configured settings\n",
        "trainer = DefaultTrainer(cfg)\n",
        "\n",
        "# Resume training if a checkpoint is found, otherwise start fresh\n",
        "trainer.resume_or_load(resume=False)\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "print(f\"Training complete. Model saved to {cfg.OUTPUT_DIR}/model_final.pth\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01/15 06:26:34 d2.engine.defaults]: Model:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc_relu1): ReLU()\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (fc_relu2): ReLU()\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "[01/15 06:26:34 d2.data.datasets.coco]: Loaded 3 images in COCO format from /content/annotations.json\n",
            "[01/15 06:26:34 d2.data.build]: Removed 0 images with no usable annotations. 3 images left.\n",
            "[01/15 06:26:34 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "[01/15 06:26:34 d2.data.build]: Using training sampler TrainingSampler\n",
            "[01/15 06:26:34 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[01/15 06:26:34 d2.data.common]: Serializing 3 elements to byte tensors and concatenating them all ...\n",
            "[01/15 06:26:34 d2.data.common]: Serialized dataset takes 0.00 MiB\n",
            "[01/15 06:26:34 d2.data.build]: Making batched data loader with batch_size=2\n",
            "WARNING [01/15 06:26:34 d2.solver.build]: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
            "[01/15 06:26:34 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "model_final_280758.pkl: 167MB [00:00, 229MB/s]                           \n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Some model parameters or buffers are not found in the checkpoint:\n",
            "roi_heads.box_predictor.bbox_pred.{bias, weight}\n",
            "roi_heads.box_predictor.cls_score.{bias, weight}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01/15 06:26:35 d2.engine.train_loop]: Starting training from iteration 0\n",
            "[01/15 06:26:42 d2.utils.events]:  eta: 0:01:40  iter: 19  total_loss: 1.336  loss_cls: 0.9786  loss_box_reg: 0.08819  loss_rpn_cls: 0.2598  loss_rpn_loc: 0.009094    time: 0.3675  last_time: 0.2923  data_time: 0.0155  last_data_time: 0.0067   lr: 1.6068e-05  max_mem: 14433M\n",
            "[01/15 06:26:50 d2.utils.events]:  eta: 0:01:33  iter: 39  total_loss: 0.7051  loss_cls: 0.3357  loss_box_reg: 0.09143  loss_rpn_cls: 0.2145  loss_rpn_loc: 0.006501    time: 0.3756  last_time: 0.2940  data_time: 0.0099  last_data_time: 0.0048   lr: 3.2718e-05  max_mem: 14433M\n",
            "[01/15 06:26:58 d2.utils.events]:  eta: 0:01:26  iter: 59  total_loss: 0.4987  loss_cls: 0.1844  loss_box_reg: 0.1141  loss_rpn_cls: 0.1739  loss_rpn_loc: 0.005904    time: 0.3736  last_time: 0.3693  data_time: 0.0071  last_data_time: 0.0089   lr: 4.9367e-05  max_mem: 14433M\n",
            "[01/15 06:27:05 d2.utils.events]:  eta: 0:01:19  iter: 79  total_loss: 0.5911  loss_cls: 0.273  loss_box_reg: 0.2216  loss_rpn_cls: 0.1146  loss_rpn_loc: 0.005646    time: 0.3688  last_time: 0.3307  data_time: 0.0059  last_data_time: 0.0052   lr: 6.6017e-05  max_mem: 14433M\n",
            "[01/15 06:27:12 d2.utils.events]:  eta: 0:01:12  iter: 99  total_loss: 0.5414  loss_cls: 0.28  loss_box_reg: 0.2238  loss_rpn_cls: 0.03704  loss_rpn_loc: 0.00601    time: 0.3668  last_time: 0.4219  data_time: 0.0063  last_data_time: 0.0065   lr: 8.2668e-05  max_mem: 14433M\n",
            "[01/15 06:27:19 d2.utils.events]:  eta: 0:01:05  iter: 119  total_loss: 0.5415  loss_cls: 0.273  loss_box_reg: 0.2436  loss_rpn_cls: 0.01478  loss_rpn_loc: 0.006438    time: 0.3676  last_time: 0.3364  data_time: 0.0056  last_data_time: 0.0046   lr: 9.9318e-05  max_mem: 14433M\n",
            "[01/15 06:27:27 d2.utils.events]:  eta: 0:00:58  iter: 139  total_loss: 0.4845  loss_cls: 0.2494  loss_box_reg: 0.2211  loss_rpn_cls: 0.01128  loss_rpn_loc: 0.004615    time: 0.3691  last_time: 0.3810  data_time: 0.0058  last_data_time: 0.0049   lr: 0.00011597  max_mem: 14433M\n",
            "[01/15 06:27:35 d2.utils.events]:  eta: 0:00:51  iter: 159  total_loss: 0.4774  loss_cls: 0.2385  loss_box_reg: 0.2235  loss_rpn_cls: 0.01263  loss_rpn_loc: 0.005316    time: 0.3714  last_time: 0.3718  data_time: 0.0070  last_data_time: 0.0062   lr: 0.00013262  max_mem: 14433M\n",
            "[01/15 06:27:42 d2.utils.events]:  eta: 0:00:43  iter: 179  total_loss: 0.4644  loss_cls: 0.2292  loss_box_reg: 0.2203  loss_rpn_cls: 0.01028  loss_rpn_loc: 0.004448    time: 0.3721  last_time: 0.3346  data_time: 0.0057  last_data_time: 0.0048   lr: 0.00014927  max_mem: 14433M\n",
            "[01/15 06:27:50 d2.utils.events]:  eta: 0:00:36  iter: 199  total_loss: 0.4819  loss_cls: 0.2172  loss_box_reg: 0.2498  loss_rpn_cls: 0.01064  loss_rpn_loc: 0.006488    time: 0.3727  last_time: 0.3755  data_time: 0.0062  last_data_time: 0.0104   lr: 0.00016592  max_mem: 14433M\n",
            "[01/15 06:27:57 d2.utils.events]:  eta: 0:00:29  iter: 219  total_loss: 0.4702  loss_cls: 0.201  loss_box_reg: 0.2654  loss_rpn_cls: 0.008322  loss_rpn_loc: 0.004749    time: 0.3715  last_time: 0.3013  data_time: 0.0061  last_data_time: 0.0053   lr: 0.00018257  max_mem: 14433M\n",
            "[01/15 06:28:04 d2.utils.events]:  eta: 0:00:21  iter: 239  total_loss: 0.5181  loss_cls: 0.1966  loss_box_reg: 0.3125  loss_rpn_cls: 0.007512  loss_rpn_loc: 0.005508    time: 0.3706  last_time: 0.3515  data_time: 0.0063  last_data_time: 0.0050   lr: 0.00019922  max_mem: 14433M\n",
            "[01/15 06:28:12 d2.utils.events]:  eta: 0:00:14  iter: 259  total_loss: 0.5447  loss_cls: 0.1821  loss_box_reg: 0.341  loss_rpn_cls: 0.006109  loss_rpn_loc: 0.005718    time: 0.3713  last_time: 0.3137  data_time: 0.0063  last_data_time: 0.0113   lr: 0.00021587  max_mem: 14433M\n",
            "[01/15 06:28:19 d2.utils.events]:  eta: 0:00:07  iter: 279  total_loss: 0.519  loss_cls: 0.1687  loss_box_reg: 0.3392  loss_rpn_cls: 0.006015  loss_rpn_loc: 0.005195    time: 0.3708  last_time: 0.4193  data_time: 0.0061  last_data_time: 0.0052   lr: 0.00023252  max_mem: 14433M\n",
            "[01/15 06:28:27 d2.utils.events]:  eta: 0:00:00  iter: 299  total_loss: 0.4993  loss_cls: 0.1544  loss_box_reg: 0.3213  loss_rpn_cls: 0.004027  loss_rpn_loc: 0.004972    time: 0.3713  last_time: 0.3572  data_time: 0.0064  last_data_time: 0.0052   lr: 0.00024917  max_mem: 14433M\n",
            "[01/15 06:28:28 d2.engine.hooks]: Overall training speed: 298 iterations in 0:01:50 (0.3713 s / it)\n",
            "[01/15 06:28:28 d2.engine.hooks]: Total training time: 0:01:52 (0:00:01 on hooks)\n",
            "Training complete. Model saved to ./output/model_final.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are assigned to build a wildlife monitoring system to detect and track\n",
        "different animal species in a forest using Detectron2. Describe the end-to-end pipeline\n",
        "from data collection to deploying the model, and how you would handle challenges like\n",
        "occlusion or nighttime detection.\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "aK-6g9UCWOoI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building a wildlife monitoring system requires a robust pipeline that can handle environmental variability (rain, fog, lighting) and biological factors (animal movement, camouflage).\n",
        "\n",
        "Stage\t                Action\t                Details\n",
        "\n",
        "Data Collection\t  Camera Traps & Drones\t      [Deploy motion-triggered infrared (IR) cameras and RGB-thermal drones to collect vast amounts of imagery.]\n",
        "\n",
        "Curation\t        Empty Frame Filtering\t       [Use a pre-screening tool (like MegaDetector) to delete the ~70% of images containing only moving grass or shadows].\n",
        "\n",
        "Annotation\t      COCO Labeling\t                [Annotate species using polygons (Instance Segmentation) rather than just boxes. This helps the model distinguish animal silhouettes from complex forest backgrounds].\n",
        "\n",
        "Training\t       Transfer Learning\t         [Use a Mask R-CNN backbone (ResNet-50 or 101) pretrained on COCO. Fine-tune specifically on forest-specific classes].\n",
        "\n",
        "Deployment\t     Edge Computing\t             [Export the model to TorchScript or ONNX to run on edge devices like NVIDIA Jetson Nano/Orin placed in waterproof forest enclosures].\n",
        "\n",
        "2. Handling Technical Challenges\n",
        "Occlusion (Animals behind trees):\n",
        "\n",
        "Solution: Use Data Augmentation during training. Specifically, apply \"Random Erasing\" or \"Cutout\" to simulate parts of the animal being hidden.\n",
        "\n",
        "Architecture: Use a Feature Pyramid Network (FPN) which detects objects at multiple scales, helping identify an animal even if only its head or tail is visible.\n",
        "\n",
        "Nighttime Detection:\n",
        "\n",
        "Solution: Include Infrared (IR) and Thermal imagery in your training set.\n",
        "\n",
        "Pre-processing: Apply CLAHE (Contrast Limited Adaptive Histogram Equalization) to nighttime frames to enhance the edges of silhouettes in low-contrast IR footage.\n",
        "\n",
        "3. Implementation Code\n",
        "\n",
        "This script demonstrates how to configure a model specifically for the \"Forest Environment\" constraints."
      ],
      "metadata": {
        "id": "2VioInTsWpZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.data import build_detection_train_loader\n",
        "from detectron2.data import transforms as T\n",
        "\n",
        "# --- 1. Custom Augmentation for Occlusion ---\n",
        "def build_forest_train_loader(cfg):\n",
        "    # Adding specific augmentations to handle trees blocking animals\n",
        "    augs = [\n",
        "        T.RandomFlip(prob=0.5),\n",
        "        T.RandomRotation(angle=[-10, 10]),\n",
        "        T.RandomLighting(0.8), # Simulates changing forest light\n",
        "        T.RandomCrop(\"relative\", (0.8, 0.8)) # Simulates partial views\n",
        "    ]\n",
        "    return build_detection_train_loader(cfg, mapper=None) # Use default mapper with these augs\n",
        "\n",
        "# --- 2. Configuration for Wildlife Model ---\n",
        "cfg = get_cfg()\n",
        "# Using Mask R-CNN for better silhouette extraction in complex backgrounds\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"))\n",
        "\n",
        "cfg.DATASETS.TRAIN = (\"wildlife_forest_train\",)\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\")\n",
        "\n",
        "# --- 3. Hyperparameters for Night/Occlusion Robustness ---\n",
        "cfg.SOLVER.IMS_PER_BATCH = 4\n",
        "cfg.SOLVER.BASE_LR = 0.001\n",
        "cfg.SOLVER.MAX_ITER = 1000\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 5 # e.g., Deer, Tiger, Boar, Elephant, Human\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.4 # Lower threshold to catch occluded animals\n",
        "\n",
        "# --- 4. Output & Training ---\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "print(\"Pipeline Configured: Augmented for occlusion and night-time IR contrast.\")\n",
        "# trainer = DefaultTrainer(cfg)\n",
        "# trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6iZPy9YYWP2",
        "outputId": "52b981e6-fcd7-417c-a7bb-14faaca2d98e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline Configured: Augmented for occlusion and night-time IR contrast.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}